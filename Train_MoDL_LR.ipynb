{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os, sys\n",
    "import logging\n",
    "import random\n",
    "import h5py\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sigpy.plot as pl\n",
    "import torch\n",
    "import sigpy as sp\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "# import custom libraries\n",
    "from utils import transforms as T\n",
    "from utils import subsample as ss\n",
    "from utils import complex_utils as cplx\n",
    "from utils.resnet2p1d import generate_model\n",
    "from utils.flare_utils import roll\n",
    "# import custom classes\n",
    "from utils.datasets import SliceData\n",
    "from subsample_fastmri import MaskFunc\n",
    "from MoDL_single import UnrolledModel\n",
    "import argparse\n",
    "from models.SAmodel import MyNetwork\n",
    "from models.Unrolled import Unrolled\n",
    "from models.UnrolledRef import UnrolledRef\n",
    "from models.UnrolledTransformer import UnrolledTrans\n",
    "import matplotlib.pyplot as plt\n",
    "from ImageFusionBlock import ImageFusionBlock\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "from models.UnrolledFusion import UnrolledFusion\n",
    "from fastmri.data import transforms, subsample\n",
    "%load_ext autoreload\n",
    "%autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    \"\"\"\n",
    "    Data Transformer for training unrolled reconstruction models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_func, args, use_seed=False):\n",
    "        self.mask_func = mask_func\n",
    "        self.use_seed = use_seed\n",
    "        self.rng = np.random.RandomState()\n",
    "    \n",
    "    def get_mask_func(self, factor):\n",
    "        center_fractions = 0.08 * 4/factor # EquiSpacedMaskFuncRandomMaskFunc\n",
    "        mask_func = subsample.RandomMaskFunc(\n",
    "        center_fractions=[center_fractions],\n",
    "        accelerations=[factor], \n",
    "        )\n",
    "        return mask_func\n",
    "    \n",
    "    def __call__(self, kspace, target, reference, reference_kspace,slice):\n",
    "        im_lowres = abs(sp.ifft(sp.resize(sp.resize(kspace,(256,24)),(256,160))))\n",
    "        magnitude_vals = im_lowres.reshape(-1)\n",
    "        k = int(round(0.05 * magnitude_vals.shape[0]))\n",
    "        scale = magnitude_vals[magnitude_vals.argsort()[::-1][k]]\n",
    "        kspace = kspace/scale\n",
    "        target = target/scale\n",
    "        # Convert everything from numpy arrays to tensors\n",
    "        kspace_torch = cplx.to_tensor(kspace).float()   \n",
    "        target_torch = cplx.to_tensor(target).float()  \n",
    "        target_torch = T.ifft2(T.kspace_cut(T.fft2(target_torch),0.67,0.67)) \n",
    "\n",
    "        kspace_torch = T.awgn_torch(kspace_torch,10,L=1)\n",
    "        ## Masking\n",
    "        mask_func = self.get_mask_func(3)\n",
    "        kspace_torch = T.kspace_cut(kspace_torch,0.67,0.67)\n",
    "        kspace_torch = transforms.apply_mask(kspace_torch, mask_func)[0]\n",
    "        # kspace_torch = kspace_torch*mask_torch # For poisson\n",
    "        \n",
    "        mask = np.abs(cplx.to_numpy(kspace_torch))!=0\n",
    "        mask_torch = torch.stack([torch.tensor(mask).float(),torch.tensor(mask).float()],dim=2)\n",
    "        \n",
    "        ### Reference addition ###\n",
    "        im_lowres_ref = abs(sp.ifft(sp.resize(sp.resize(reference_kspace,(256,24)),(256,160))))\n",
    "        magnitude_vals_ref = im_lowres_ref.reshape(-1)\n",
    "        k_ref = int(round(0.05 * magnitude_vals_ref.shape[0]))\n",
    "        scale_ref = magnitude_vals_ref[magnitude_vals_ref.argsort()[::-1][k_ref]]\n",
    "        reference = reference / scale_ref\n",
    "        reference_torch = cplx.to_tensor(reference).float()\n",
    "        reference_torch_kspace = T.fft2(reference_torch)\n",
    "        reference_torch_kspace = T.kspace_cut(reference_torch_kspace,0.67,0.67)\n",
    "        reference_torch = T.ifft2(reference_torch_kspace)\n",
    "        \n",
    "\n",
    "        return kspace_torch,target_torch,mask_torch, reference_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    # Generate k-t undersampling masks\n",
    "    train_mask = MaskFunc([0.08],[4])\n",
    "    train_data = SliceData(\n",
    "        root=str(args.data_path),\n",
    "        transform=DataTransform(train_mask, args),\n",
    "        sample_rate=1\n",
    "    )\n",
    "    return train_data\n",
    "def create_data_loaders(args):\n",
    "    train_data = create_datasets(args)\n",
    "#     print(train_data[0])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader\n",
    "def build_optim(args, params):\n",
    "    optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "params = Namespace()\n",
    "params.data_path = \"./registered_data/\"\n",
    "params.batch_size = 8\n",
    "params.num_grad_steps = 4 #4\n",
    "params.num_cg_steps = 8 #8\n",
    "params.share_weights = True\n",
    "params.modl_lamda = 0.05\n",
    "params.lr = 0.00001\n",
    "params.weight_decay = 0\n",
    "params.lr_step_size = 10\n",
    "params.lr_gamma = 0.3\n",
    "params.epoch = 61\n",
    "params.reference_mode = 0\n",
    "params.reference_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tal/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_data_loaders(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights\n"
     ]
    }
   ],
   "source": [
    "\n",
    "single_MoDL = UnrolledModel(params).to(device)\n",
    "optimizer = build_optim(params, single_MoDL.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, params.lr_step_size, params.lr_gamma)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch = [  0/ 61] Iter = [   0/  67] Loss = 0.01383 Avg Loss = 0.01383\n",
      "INFO:root:Epoch = [  1/ 61] Iter = [   0/  67] Loss = 0.01241 Avg Loss = 0.01241\n",
      "INFO:root:Epoch = [  2/ 61] Iter = [   0/  67] Loss = 0.01139 Avg Loss = 0.01139\n",
      "INFO:root:Epoch = [  3/ 61] Iter = [   0/  67] Loss = 0.01103 Avg Loss = 0.01103\n",
      "INFO:root:Epoch = [  4/ 61] Iter = [   0/  67] Loss = 0.0103 Avg Loss = 0.0103\n",
      "INFO:root:Epoch = [  5/ 61] Iter = [   0/  67] Loss = 0.01015 Avg Loss = 0.01015\n",
      "INFO:root:Epoch = [  6/ 61] Iter = [   0/  67] Loss = 0.009444 Avg Loss = 0.009444\n",
      "INFO:root:Epoch = [  7/ 61] Iter = [   0/  67] Loss = 0.008828 Avg Loss = 0.008828\n",
      "INFO:root:Epoch = [  8/ 61] Iter = [   0/  67] Loss = 0.009559 Avg Loss = 0.009559\n",
      "INFO:root:Epoch = [  9/ 61] Iter = [   0/  67] Loss = 0.009308 Avg Loss = 0.009308\n",
      "INFO:root:Epoch = [ 10/ 61] Iter = [   0/  67] Loss = 0.009014 Avg Loss = 0.009014\n",
      "INFO:root:Epoch = [ 11/ 61] Iter = [   0/  67] Loss = 0.009135 Avg Loss = 0.009135\n",
      "INFO:root:Epoch = [ 12/ 61] Iter = [   0/  67] Loss = 0.01028 Avg Loss = 0.01028\n",
      "INFO:root:Epoch = [ 13/ 61] Iter = [   0/  67] Loss = 0.01005 Avg Loss = 0.01005\n",
      "INFO:root:Epoch = [ 14/ 61] Iter = [   0/  67] Loss = 0.008727 Avg Loss = 0.008727\n",
      "INFO:root:Epoch = [ 15/ 61] Iter = [   0/  67] Loss = 0.009077 Avg Loss = 0.009077\n",
      "INFO:root:Epoch = [ 16/ 61] Iter = [   0/  67] Loss = 0.01001 Avg Loss = 0.01001\n",
      "INFO:root:Epoch = [ 17/ 61] Iter = [   0/  67] Loss = 0.008579 Avg Loss = 0.008579\n",
      "INFO:root:Epoch = [ 18/ 61] Iter = [   0/  67] Loss = 0.008844 Avg Loss = 0.008844\n",
      "INFO:root:Epoch = [ 19/ 61] Iter = [   0/  67] Loss = 0.008788 Avg Loss = 0.008788\n",
      "INFO:root:Epoch = [ 20/ 61] Iter = [   0/  67] Loss = 0.009849 Avg Loss = 0.009849\n",
      "INFO:root:Epoch = [ 21/ 61] Iter = [   0/  67] Loss = 0.008283 Avg Loss = 0.008283\n",
      "INFO:root:Epoch = [ 22/ 61] Iter = [   0/  67] Loss = 0.009626 Avg Loss = 0.009626\n",
      "INFO:root:Epoch = [ 23/ 61] Iter = [   0/  67] Loss = 0.009479 Avg Loss = 0.009479\n",
      "INFO:root:Epoch = [ 24/ 61] Iter = [   0/  67] Loss = 0.008728 Avg Loss = 0.008728\n",
      "INFO:root:Epoch = [ 25/ 61] Iter = [   0/  67] Loss = 0.008013 Avg Loss = 0.008013\n",
      "INFO:root:Epoch = [ 26/ 61] Iter = [   0/  67] Loss = 0.008591 Avg Loss = 0.008591\n",
      "INFO:root:Epoch = [ 27/ 61] Iter = [   0/  67] Loss = 0.008589 Avg Loss = 0.008589\n",
      "INFO:root:Epoch = [ 28/ 61] Iter = [   0/  67] Loss = 0.008456 Avg Loss = 0.008456\n",
      "INFO:root:Epoch = [ 29/ 61] Iter = [   0/  67] Loss = 0.009022 Avg Loss = 0.009022\n",
      "INFO:root:Epoch = [ 30/ 61] Iter = [   0/  67] Loss = 0.009464 Avg Loss = 0.009464\n",
      "INFO:root:Epoch = [ 31/ 61] Iter = [   0/  67] Loss = 0.009128 Avg Loss = 0.009128\n",
      "INFO:root:Epoch = [ 32/ 61] Iter = [   0/  67] Loss = 0.008061 Avg Loss = 0.008061\n",
      "INFO:root:Epoch = [ 33/ 61] Iter = [   0/  67] Loss = 0.008595 Avg Loss = 0.008595\n",
      "INFO:root:Epoch = [ 34/ 61] Iter = [   0/  67] Loss = 0.008114 Avg Loss = 0.008114\n",
      "INFO:root:Epoch = [ 35/ 61] Iter = [   0/  67] Loss = 0.008761 Avg Loss = 0.008761\n",
      "INFO:root:Epoch = [ 36/ 61] Iter = [   0/  67] Loss = 0.008674 Avg Loss = 0.008674\n",
      "INFO:root:Epoch = [ 37/ 61] Iter = [   0/  67] Loss = 0.008022 Avg Loss = 0.008022\n",
      "INFO:root:Epoch = [ 38/ 61] Iter = [   0/  67] Loss = 0.008383 Avg Loss = 0.008383\n",
      "INFO:root:Epoch = [ 39/ 61] Iter = [   0/  67] Loss = 0.008301 Avg Loss = 0.008301\n",
      "INFO:root:Epoch = [ 40/ 61] Iter = [   0/  67] Loss = 0.008673 Avg Loss = 0.008673\n",
      "INFO:root:Epoch = [ 41/ 61] Iter = [   0/  67] Loss = 0.008484 Avg Loss = 0.008484\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m reference \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 33\u001b[0m im_out \u001b[38;5;241m=\u001b[39m \u001b[43msingle_MoDL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreference_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(im_out,target)\n\u001b[1;32m     35\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m+\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/docker/MoDLsinglechannel/modl_singlechannel_reference/MoDL_single.py:132\u001b[0m, in \u001b[0;36mUnrolledModel.forward\u001b[0;34m(self, kspace, reference_image, init_image, mask)\u001b[0m\n\u001b[1;32m    130\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m zf_image \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodl_lamda \u001b[38;5;241m*\u001b[39m image\n\u001b[1;32m    131\u001b[0m     CG_alg \u001b[38;5;241m=\u001b[39m ConjGrad(Aop_fun\u001b[38;5;241m=\u001b[39mSense\u001b[38;5;241m.\u001b[39mnormal,b\u001b[38;5;241m=\u001b[39mrhs,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,l2lam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodl_lamda,max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cg_steps)\n\u001b[0;32m--> 132\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mCG_alg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/docker/MoDLsinglechannel/modl_singlechannel_reference/utils/flare_utils.py:174\u001b[0m, in \u001b[0;36mConjGrad.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m#return MyConjGrad(self.b, self.Aop_fun, self.max_iter, self.l2lam, self.eps, True)(x)\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconjgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAop_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAop_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2lam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml2lam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/docker/MoDLsinglechannel/modl_singlechannel_reference/utils/flare_utils.py:202\u001b[0m, in \u001b[0;36mconjgrad\u001b[0;34m(x, b, Aop_fun, max_iter, l2lam, eps, verbose)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{rsnew}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m=\u001b[39mi, rsnew\u001b[38;5;241m=\u001b[39mdbp\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mitemize(torch\u001b[38;5;241m.\u001b[39msqrt(rsnew))))\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rsnew\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m eps:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l2lam \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress specific deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Function sporco.util.tikhonov_filter is deprecated.*\")\n",
    "\n",
    "# Suppress specific user warnings from torchvision.models\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Using 'weights' as positional parameter.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Arguments other than a weight enum or `None` for 'weights' are deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, \n",
    "                        message=\".*Function sporco.util.tikhonov_filter is deprecated; please use function sporco.signal.tikhonov_filter instead.*\")\n",
    "\n",
    "epochs_plot = []\n",
    "losses_plot = []\n",
    "\n",
    "for epoch in range(params.epoch):\n",
    "    single_MoDL.train()\n",
    "    avg_loss = 0.\n",
    "    running_loss = 0.0\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        input,target,mask,reference = data\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        mask = mask.to(device)\n",
    "        reference = reference.to(device)\n",
    "\n",
    "        im_out = single_MoDL(input.float(),reference_image=reference,mask=mask)\n",
    "        loss = criterion(im_out,target)\n",
    "        running_loss = running_loss + loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        if iter % 125 == 0:\n",
    "            logging.info(\n",
    "                f'Epoch = [{epoch:3d}/{params.epoch:3d}] '\n",
    "                f'Iter = [{iter:4d}/{len(train_loader):4d}] '\n",
    "                f'Loss = {loss.item():.4g} Avg Loss = {avg_loss:.4g}'\n",
    "            )\n",
    "    #Saving the model\n",
    "    exp_dir = \"L2_checkpoints_poisson_x2_MoDL_horizontal_LR_grant/\"\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'params': params,\n",
    "                'model': single_MoDL.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'exp_dir': exp_dir\n",
    "            },\n",
    "            f=os.path.join(exp_dir, 'model_%d.pt'%(epoch))\n",
    "    )\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "    #scheduler.step(running_loss)\n",
    "    scheduler.step()\n",
    "    # Append epoch and average loss to plot lists\n",
    "    epochs_plot.append(epoch)\n",
    "    losses_plot.append(running_loss)\n",
    "\n",
    "# Plotting the loss curve\n",
    "plt.figure()\n",
    "plt.plot(epochs_plot, losses_plot, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SA unrolled with Reference L2 train Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(exp_dir, 'loss_plot_plato_down.png'))  # Save plot as an image\n",
    "\n",
    "# Save all_losses to a file for later comparison\n",
    "losses_file = os.path.join(exp_dir, 'all_losses.txt')\n",
    "with open(losses_file, 'w') as f:\n",
    "    for loss in losses_plot:\n",
    "        f.write(f'{loss}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
